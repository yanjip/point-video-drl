# 5.10
- 要考虑dis的影响，可能会直接降低了zi的权重比例。
- dis_range=[1,1.5,2.0],去倒数就是0.5-1.0
- zi的范围是0-1
- 版本质量l的取值范围是1-5

1. 现在的问题是：环境依赖于每次的Fov提供的tile信息，如何减少这种特定环境下的过拟合呢
（好像又没有）

2. 先固定Pmax，求maxQoE；

# 5.11
- 如果超过了计算能力，不done掉，作为惩罚项
- 若Tu和Td大于了Tslot，也只做惩罚，不done掉
- done是正常结束，terminal是发生特殊情况下的终止；done没有nextstate，但terminal有

# 5.12
- rt:1800Mps
- Dmax:60Mbit
- 压缩后25Mbit，未压缩30Mbit

# 5.13
- 结果action始终是[0,1]，网络根本没学到策略

# 5.14
- 可以强调一下算法的可容错性和弹性能力，因为即使违背了Tu+Td的约束条件，
凭借用户的视频缓冲区内容不为空，依旧不影响用户体验。
- 那需要将Bt看做环境吗，我觉得需要。
- 现在有两种选择，违背约束直接丢弃，或者让他继续发送。

# 5.15
- 计算出来的q值全是nan ：
解决办法：①改激活函数为softmax（没用）
②直接原因是因为next state设置为了nan，改成全0

- 又遇到问题：刚开始测试的时候，就一直action为3
原因：奖励函数没设置对

- 把softmax改回relu又可以学习到策略了

- 关于Bt，如果在某个时隙传输失败，我设定丢弃该tile，因为有Bt缓冲区的存在
仍然不会影响到用户体验。

- 现在仿真不需要把bt作为状态变量！！

- 又出现问题：总是选择质量版本最低那个action要么为4，要么9

# 5.17日
- 改了QoE的计算方法
- 每1k个step之后总是先奖励值会下降：
解决办法--删掉evaluate函数即可！为什么？我也不知道

# 5.19日
- 今天写了baseline，用贪婪算法实现（在evaluate里面）
先预分配所有tile最低的质量，然后依次分配高质量，直到达到资源上限结束
版本的进一步提高。
- 发现一个现象：给tile分配的压缩数量越少，QoE却提高了（在baseline是这样）
- 原因应该是这样：参数设置的不对，导致计算能力不够，通信资源过剩，选择压缩版本反而占用了时隙的时间。

# 5.22日
- 下层策略返回Q给上层，然后可以作为环境给上层agent，然后奖励可以
引导agent不仅max所有QoE，并且保证公平性。

# 5.23日
- 后期可以加上buffer的上限（比如联邦学习提到的3s）

# 6.4日
- 先完成基本的波束赋形，state只加sinr，后期可以考虑加上所有用户累积的QoE

